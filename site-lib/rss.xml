<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[üé® Homepage]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>site-lib/media/favicon.png</url><title>üé® Homepage</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 24 Feb 2025 15:21:02 GMT</lastBuildDate><atom:link href="site-lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 24 Feb 2025 15:20:57 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[NeuralPlane]]></title><description><![CDATA[ <a href="https://timber-ye.github.io/home.html" target="_self" rel="noopener nofollow" class="is-unresolved">Hanqiao Ye<sup></sup></a>1,2, <a href="https://paulliuyz.github.io/" target="_self" rel="noopener nofollow" class="is-unresolved">Yuzhou Liu<sup></sup></a>1,2, <a href="http://3dv.ac.cn/en/faculty/ydl/" target="_self" rel="noopener nofollow" class="is-unresolved">Yangdong Liu<sup></sup></a>2, <a href="http://3dv.ac.cn/en/faculty/shs/" target="_self" rel="noopener nofollow" class="is-unresolved">Shuhan Shen<sup></sup></a>1,2
<br>
<a href="https://www.ucas.ac.cn/" target="_self" rel="noopener nofollow" class="is-unresolved"><sup></sup> School of Artificial Intelligence, University of Chinese Academy of Sciences</a>1 , <a href="http://english.ia.cas.cn/" target="_self" rel="noopener nofollow" class="is-unresolved"><sup></sup> Institute of Automation, Chinese Academy of Sciences</a>2 <br> <a data-tooltip-position="top" aria-label="https://openreview.net/pdf?id=5UKrnKuspb" rel="noopener nofollow" class="external-link is-unresolved" href="https://openreview.net/pdf?id=5UKrnKuspb" target="_self"><code></code></a>Paper&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a data-tooltip-position="top" aria-label="https://openreview.net/attachment?id=5UKrnKuspb&amp;name=supplementary_material" rel="noopener nofollow" class="external-link is-unresolved" href="https://openreview.net/attachment?id=5UKrnKuspb&amp;name=supplementary_material" target="_self"><code></code></a>Supplementary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a data-tooltip-position="top" aria-label="https://github.com/3dv-casia/NeuralPlane" rel="noopener nofollow" class="external-link is-unresolved" href="https://github.com/3dv-casia/NeuralPlane" target="_self" style="margin-bottom: 0px;"><code style="margin-bottom: 0px;"></code></a>Code (To be uploaded) NeuralPlane rebuilds indoor scenes as arrangements of planar primitives. <br> <img style="width: 75%;" alt="Teaser" src="projects/neuralplane/assets/teaser.png" referrerpolicy="no-referrer" target="_self"> 3D maps assembled from planar primitives are compact and expressive in representing man-made environments, making them suitable for a spectrum of applications. In this paper, we present NeuralPlane, a novel approach that explores neural fields for multi-view 3D plane reconstruction. Our method is centered upon the core idea of distilling geometric and semantic cues from inconsistent 2D plane observations into a unified 3D neural representation, which unlocks the full leverage of plane attributes. NeuralPlane realizes this idea via several key designs, including: 1) a monocular module that generates geometrically smooth and semantically meaningful segments as 2D plane observations, 2) a plane-guided training procedure that implicitly learns accurate plane locations from multi-view plane observations, and 3) a self-supervised feature field termed Neural Coplanarity Field that enables the modeling of scene semantics alongside the geometry. Without relying on plane annotations, our method achieves high-fidelity reconstruction comprising planar primitives that are not only crisp but also well-aligned with the semantic content. Comprehensive experiments on ScanNetv2 and ScanNet++ demonstrate the superiority of our results in both geometry and semantics ReferenceChoose a result from PlanarRecon ObjectSDF++AirPlanesOursReferenceChoose a result from PlanarRecon ObjectSDF++AirPlanesOursReferenceChoose a result from PlanarRecon ObjectSDF++AirPlanesOursReferenceChoose a result from PlanarRecon ObjectSDF++AirPlanesOurs@inproceedings{ ye2025neuralplane, title={NeuralPlane: Structured 3D Reconstruction in Planar Primitives with Neural Fields}, author={Hanqiao Ye and Yuzhou Liu and Yangdong Liu and Shuhan Shen}, booktitle={The Thirteenth International Conference on Learning Representations}, year={2025}, url={https://openreview.net/forum?id=5UKrnKuspb}
}
]]></description><link>projects/neuralplane.html</link><guid isPermaLink="false">Projects/NeuralPlane.md</guid><pubDate>Mon, 24 Feb 2025 15:20:29 GMT</pubDate><enclosure url="projects/neuralplane/assets/teaser.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="projects/neuralplane/assets/teaser.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[video_compare]]></title><link>projects/neuralplane/video_compare.html</link><guid isPermaLink="false">Projects/NeuralPlane/video_compare.html</guid><pubDate>Mon, 24 Feb 2025 14:51:06 GMT</pubDate></item><item><title><![CDATA[results_gallery]]></title><link>projects/neuralplane/results_gallery.html</link><guid isPermaLink="false">Projects/NeuralPlane/results_gallery.html</guid><pubDate>Mon, 24 Feb 2025 14:46:31 GMT</pubDate></item><item><title><![CDATA[NeuralPlane]]></title><description><![CDATA[ <a href="https://timber-ye.github.io/home.html" target="_self" rel="noopener nofollow" class="is-unresolved">Hanqiao Ye<sup></sup></a>1,2, <a href="https://paulliuyz.github.io/" target="_self" rel="noopener nofollow" class="is-unresolved">Yuzhou Liu<sup></sup></a>1,2, <a href="http://3dv.ac.cn/en/faculty/ydl/" target="_self" rel="noopener nofollow" class="is-unresolved">Yangdong Liu<sup></sup></a>2, <a href="http://3dv.ac.cn/en/faculty/shs/" target="_self" rel="noopener nofollow" class="is-unresolved">Shuhan Shen<sup></sup></a>1,2
<br>
<a href="https://www.ucas.ac.cn/" target="_self" rel="noopener nofollow" class="is-unresolved"><sup></sup> School of Artificial Intelligence, University of Chinese Academy of Sciences</a>1 , <a href="http://english.ia.cas.cn/" target="_self" rel="noopener nofollow" class="is-unresolved"><sup></sup> Institute of Automation, Chinese Academy of Sciences</a>2 <br> <a data-tooltip-position="top" aria-label="https://openreview.net/pdf?id=5UKrnKuspb" rel="noopener nofollow" class="external-link is-unresolved" href="https://openreview.net/pdf?id=5UKrnKuspb" target="_self"><code></code></a>Paper&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a data-tooltip-position="top" aria-label="https://openreview.net/attachment?id=5UKrnKuspb&amp;name=supplementary_material" rel="noopener nofollow" class="external-link is-unresolved" href="https://openreview.net/attachment?id=5UKrnKuspb&amp;name=supplementary_material" target="_self"><code></code></a>Supplementary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a data-tooltip-position="top" aria-label="https://github.com/3dv-casia/NeuralPlane" rel="noopener nofollow" class="external-link is-unresolved" href="https://github.com/3dv-casia/NeuralPlane" target="_self" style="margin-bottom: 0px;"><code style="margin-bottom: 0px;"></code></a>Code (To be uploaded) NeuralPlane rebuilds indoor scenes as arrangements of planar primitives. <br> <img style="width: 75%;" alt="Teaser" src="projects/neuralplane/assets/teaser.png" referrerpolicy="no-referrer" target="_self"> 3D maps assembled from planar primitives are compact and expressive in representing man-made environments, making them suitable for a spectrum of applications. In this paper, we present NeuralPlane, a novel approach that explores neural fields for multi-view 3D plane reconstruction. Our method is centered upon the core idea of distilling geometric and semantic cues from inconsistent 2D plane observations into a unified 3D neural representation, which unlocks the full leverage of plane attributes. NeuralPlane realizes this idea via several key designs, including: 1) a monocular module that generates geometrically smooth and semantically meaningful segments as 2D plane observations, 2) a plane-guided training procedure that implicitly learns accurate plane locations from multi-view plane observations, and 3) a self-supervised feature field termed Neural Coplanarity Field that enables the modeling of scene semantics alongside the geometry. Without relying on plane annotations, our method achieves high-fidelity reconstruction comprising planar primitives that are not only crisp but also well-aligned with the semantic content. Comprehensive experiments on ScanNetv2 and ScanNet++ demonstrate the superiority of our results in both geometry and semantics ReferenceChoose a result from OursReferenceChoose a result from OursReferenceChoose a result from OursReferenceChoose a result from Ours@inproceedings{ ye2025neuralplane, title={NeuralPlane: Structured 3D Reconstruction in Planar Primitives with Neural Fields}, author={Hanqiao Ye and Yuzhou Liu and Yangdong Liu and Shuhan Shen}, booktitle={The Thirteenth International Conference on Learning Representations}, year={2025}, url={https://openreview.net/forum?id=5UKrnKuspb}
}
]]></description><link>projects/neuralplane/neuralplane.html</link><guid isPermaLink="false">Projects/NeuralPlane/NeuralPlane.md</guid><pubDate>Mon, 24 Feb 2025 12:46:52 GMT</pubDate><enclosure url="projects/neuralplane/assets/teaser.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="projects/neuralplane/assets/teaser.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[teaser]]></title><description><![CDATA[<img src="projects/neuralplane/assets/teaser.png" target="_self">]]></description><link>projects/neuralplane/assets/teaser.html</link><guid isPermaLink="false">Projects/NeuralPlane/assets/teaser.png</guid><pubDate>Tue, 01 Oct 2024 07:30:20 GMT</pubDate><enclosure url="projects/neuralplane/assets/teaser.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="projects/neuralplane/assets/teaser.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Home]]></title><description><![CDATA[<img src="images/home-banner.png" alt="Banner" draggable="false" aria-hidden="true" class="svelte-frmt02 gradient draggable" style="object-position: 50% 41.3%;" target="_self"> <br>Hi there üëã , this is YE HANQIAO (/yeh han chow/, Âè∂Áø∞Ê®µ ), a third-year PhD student at School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS). I am conducting reseach at 3DV Lab of <a data-tooltip-position="top" aria-label="http://english.ia.cas.cn/" rel="noopener nofollow" class="external-link is-unresolved" href="http://english.ia.cas.cn/" target="_self">Institute of Automation, Chinese Academy of Sciences (CASIA)</a> supervised by <a data-tooltip-position="top" aria-label="http://3dv.ac.cn/en/faculty/shs/" rel="noopener nofollow" class="external-link is-unresolved" href="http://3dv.ac.cn/en/faculty/shs/" target="_self">Professor Shuhan Shen</a>. Prior to that, I received my B.E. degree from <a data-tooltip-position="top" aria-label="http://en.hit.edu.cn/" rel="noopener nofollow" class="external-link is-unresolved" href="http://en.hit.edu.cn/" target="_self">Harbin Institute of Technology, Shenzhen (HITSZ)</a> in 2022, advised by <a data-tooltip-position="top" aria-label="https://pwalclab.com/public/index.php?title=Main_Page#Principle_Investigator" rel="noopener nofollow" class="external-link is-unresolved" href="https://pwalclab.com/public/index.php?title=Main_Page#Principle_Investigator" target="_self">Professor Jun Xu</a>.
My research interest lies in the 3D Computer Vision, particularly including 3D representation, pose estimation and scene understanding, as well as their applications in mixed reality and robotics. Currently, I am deeply involved in specific projects related to visual localization. If you are interested in my work, please feel free to drop me an e-mail.<img src="images/portrait.png" target="_self"><br>
<a data-tooltip-position="top" aria-label="mailto:yehanqiao2022@ia.ac.cn" rel="noopener nofollow" class="external-link" href=".html" target="_self"><span style="font-size: 30px"><span class="cm-iconize-icon" aria-label="LiMail" data-icon="LiMail" aria-hidden="true" style="display: inline-flex; transform: translateY(13%);"><svg xmlns="http://www.w3.org/2000/svg" width="30px" height="30px" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-mail"><rect x="2" y="4" width="20" height="16" rx="2"></rect><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path></svg></span></span></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a data-tooltip-position="top" aria-label="https://scholar.google.com/citations?user=jlW6mJsAAAAJ&amp;hl=en" rel="noopener nofollow" class="external-link is-unresolved" href="https://scholar.google.com/citations?user=jlW6mJsAAAAJ&amp;hl=en" target="_self"><span style="font-size: 30px"><span class="cm-iconize-icon" aria-label="LiGraduationCap" data-icon="LiGraduationCap" aria-hidden="true" style="display: inline-flex; transform: translateY(13%);"><svg xmlns="http://www.w3.org/2000/svg" width="30px" height="30px" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-graduation-cap"><path d="M21.42 10.922a1 1 0 0 0-.019-1.838L12.83 5.18a2 2 0 0 0-1.66 0L2.6 9.08a1 1 0 0 0 0 1.832l8.57 3.908a2 2 0 0 0 1.66 0z"></path><path d="M22 10v6"></path><path d="M6 12.5V16a6 3 0 0 0 12 0v-3.5"></path></svg></span></span></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a data-tooltip-position="top" aria-label="https://maps.app.goo.gl/7Dmo2SxnK6xGmRU18" rel="noopener nofollow" class="external-link is-unresolved" href="https://maps.app.goo.gl/7Dmo2SxnK6xGmRU18" target="_self" style="margin-bottom: 0px;"><span style="font-size: 30px; margin-bottom: 0px;"><span class="cm-iconize-icon" aria-label="LiMapPinned" data-icon="LiMapPinned" aria-hidden="true" style="display: inline-flex; transform: translateY(13%);"><svg xmlns="http://www.w3.org/2000/svg" width="30px" height="30px" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-map-pinned"><path d="M18 8c0 3.613-3.869 7.429-5.393 8.795a1 1 0 0 1-1.214 0C9.87 15.429 6 11.613 6 8a6 6 0 0 1 12 0"></path><circle cx="12" cy="8" r="2"></circle><path d="M8.714 14h-3.71a1 1 0 0 0-.948.683l-2.004 6A1 1 0 0 0 3 22h18a1 1 0 0 0 .948-1.316l-2-6a1 1 0 0 0-.949-.684h-3.712"></path></svg></span></span></a>
[2025/02] One paper is to be presented as an Oral in ICLR 2025 ü•≥.
[2024/07] A co-authored paper gets accepted in ECCV 2024.
<br>[2024/06] We win first place in&nbsp;the <a data-tooltip-position="top" aria-label="https://usm3d.github.io/" rel="noopener nofollow" class="external-link is-unresolved" href="https://usm3d.github.io/" target="_self">1st Building3D challenge in CVPR 2024 Workshop on Urban Scene Modeling</a>.
[2024/04] A co-authored paper submitted to IEEE RA-L gets accepted.
WarningThis webpage is VERY much a work in progress. As such it is incomplete.]]></description><link>home.html</link><guid isPermaLink="false">Home.md</guid><pubDate>Sat, 22 Feb 2025 07:59:45 GMT</pubDate><enclosure url="images/home-banner.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="images/home-banner.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[home-banner]]></title><description><![CDATA[<img src="images/home-banner.png" target="_self">]]></description><link>images/home-banner.html</link><guid isPermaLink="false">Images/home-banner.png</guid><pubDate>Sat, 22 Feb 2025 07:42:19 GMT</pubDate><enclosure url="images/home-banner.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="images/home-banner.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Portrait]]></title><description><![CDATA[<img src="images/portrait.png" target="_self">]]></description><link>images/portrait.html</link><guid isPermaLink="false">Images/Portrait.png</guid><pubDate>Fri, 21 Feb 2025 08:50:13 GMT</pubDate><enclosure url="images/portrait.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="images/portrait.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[teaser-video]]></title><link>projects/neuralplane/assets/teaser-video.html</link><guid isPermaLink="false">Projects/NeuralPlane/assets/teaser-video.mp4</guid><pubDate>Sat, 28 Sep 2024 05:29:01 GMT</pubDate></item><item><title><![CDATA[bulma.css.map]]></title><link>projects/neuralplane/static/css/bulma.css.map.html</link><guid isPermaLink="false">Projects/NeuralPlane/static/css/bulma.css.map.txt</guid><pubDate>Tue, 24 Sep 2024 01:42:53 GMT</pubDate></item><item><title><![CDATA[index]]></title><link>projects/neuralplane/index.html</link><guid isPermaLink="false">Projects/NeuralPlane/index.html</guid><pubDate>Mon, 24 Feb 2025 08:49:02 GMT</pubDate></item></channel></rss>